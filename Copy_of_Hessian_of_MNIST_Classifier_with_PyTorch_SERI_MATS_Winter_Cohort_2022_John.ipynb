{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nmonson1/mnist_exploration/blob/main/Copy_of_Hessian_of_MNIST_Classifier_with_PyTorch_SERI_MATS_Winter_Cohort_2022_John.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MNIST Classifier with PyTorch - Finding the Hessian"
      ],
      "metadata": {
        "id": "mXjTUeGTxx0X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing dependencies"
      ],
      "metadata": {
        "id": "nsj9tNFiyh0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "6NT89v_EyKre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.tensor([2,3,4])\n",
        "y=torch.tensor([1,2,3])\n",
        "z=x+y\n",
        "z=2*z\n",
        "print(z.grad_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVpkUQWCmCMl",
        "outputId": "23d2b784-c922-4705-ab24-32b75614ff1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network"
      ],
      "metadata": {
        "id": "ETJ7AXWXyqPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 1, 3, 1)\n",
        "    self.fc2 = nn.Linear(169, 10)\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "    if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n",
        "      module.weight.data.normal_(mean=0.0, std=0.5)\n",
        "      if module.bias is not None:\n",
        "        module.bias.data.zero_()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = F.relu(x)\n",
        "    x = F.max_pool2d(x, 2)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.fc2(x)\n",
        "    output = F.log_softmax(x, dim=1)\n",
        "    return output"
      ],
      "metadata": {
        "id": "gQkfp68_yuTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters"
      ],
      "metadata": {
        "id": "Y4DSU0psywaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "TEST_BATCH_SIZE = 1000\n",
        "EPOCHS = 10\n",
        "LR = 1.0\n",
        "GAMMA = 0.7\n",
        "num_params = 1710"
      ],
      "metadata": {
        "id": "gepaQsrJy5qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "if use_cuda:\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "else:\n",
        "    DEVICE = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "OxhkYHwueZOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating model, loss function, and optimizer"
      ],
      "metadata": {
        "id": "l6pU2jcGy72-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net().to(DEVICE)\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=LR)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "BkeUHnnYzGdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading data"
      ],
      "metadata": {
        "id": "Mis9k5WyzIPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_kwargs = {'batch_size': BATCH_SIZE}\n",
        "test_kwargs = {'batch_size': TEST_BATCH_SIZE}\n",
        "if use_cuda:\n",
        "  cuda_kwargs = {'num_workers': 1, 'pin_memory': True, 'shuffle': True}\n",
        "  train_kwargs.update(cuda_kwargs)\n",
        "  test_kwargs.update(cuda_kwargs)\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "dataset1 = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
        "dataset2 = datasets.MNIST('../data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(Subset(dataset1, np.random.randint(len(dataset1), size=1500)), **train_kwargs)  # subset of datasets to keep network overparameterized\n",
        "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)"
      ],
      "metadata": {
        "id": "qYI0TY9izLxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and test functions"
      ],
      "metadata": {
        "id": "UQchiB57zNnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dry_run: bool, model, device, train_loader, optimizer, epoch):\n",
        "  model.train()\n",
        "\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = F.nll_loss(output, target)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch_idx % 10 == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item()))\n",
        "      if dry_run:\n",
        "        break\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "  model.eval()\n",
        "\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      output = model(data)\n",
        "      test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "      pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "id": "K8cy_2WNzUwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model"
      ],
      "metadata": {
        "id": "lHUVXXWqzVfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler = StepLR(optimizer, step_size=1, gamma=GAMMA)\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  train(False, model, DEVICE, train_loader, optimizer, epoch)\n",
        "  test(model, DEVICE, test_loader)\n",
        "  scheduler.step()\n",
        "\n",
        "torch.save(model.state_dict(), \"mnist_cnn_small.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Orb-sGE4zY7F",
        "outputId": "06395cc4-1b8e-4430-cae3-4c7bcc20cbfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/1500 (0%)]\tLoss: 14.053327\n",
            "Train Epoch: 1 [640/1500 (42%)]\tLoss: 7.843325\n",
            "Train Epoch: 1 [1280/1500 (83%)]\tLoss: 6.455245\n",
            "\n",
            "Test set: Average loss: 5.0164, Accuracy: 1221/10000 (12%)\n",
            "\n",
            "Train Epoch: 2 [0/1500 (0%)]\tLoss: 5.542170\n",
            "Train Epoch: 2 [640/1500 (42%)]\tLoss: 5.029972\n",
            "Train Epoch: 2 [1280/1500 (83%)]\tLoss: 3.821646\n",
            "\n",
            "Test set: Average loss: 3.9806, Accuracy: 1480/10000 (15%)\n",
            "\n",
            "Train Epoch: 3 [0/1500 (0%)]\tLoss: 4.083921\n",
            "Train Epoch: 3 [640/1500 (42%)]\tLoss: 3.941819\n",
            "Train Epoch: 3 [1280/1500 (83%)]\tLoss: 3.859973\n",
            "\n",
            "Test set: Average loss: 3.6049, Accuracy: 1703/10000 (17%)\n",
            "\n",
            "Train Epoch: 4 [0/1500 (0%)]\tLoss: 3.851332\n",
            "Train Epoch: 4 [640/1500 (42%)]\tLoss: 3.697637\n",
            "Train Epoch: 4 [1280/1500 (83%)]\tLoss: 3.568192\n",
            "\n",
            "Test set: Average loss: 3.4062, Accuracy: 1856/10000 (19%)\n",
            "\n",
            "Train Epoch: 5 [0/1500 (0%)]\tLoss: 3.473230\n",
            "Train Epoch: 5 [640/1500 (42%)]\tLoss: 3.070641\n",
            "Train Epoch: 5 [1280/1500 (83%)]\tLoss: 3.483856\n",
            "\n",
            "Test set: Average loss: 3.2768, Accuracy: 1952/10000 (20%)\n",
            "\n",
            "Train Epoch: 6 [0/1500 (0%)]\tLoss: 3.299085\n",
            "Train Epoch: 6 [640/1500 (42%)]\tLoss: 3.041352\n",
            "Train Epoch: 6 [1280/1500 (83%)]\tLoss: 2.980655\n",
            "\n",
            "Test set: Average loss: 3.1874, Accuracy: 2017/10000 (20%)\n",
            "\n",
            "Train Epoch: 7 [0/1500 (0%)]\tLoss: 3.099020\n",
            "Train Epoch: 7 [640/1500 (42%)]\tLoss: 2.830901\n",
            "Train Epoch: 7 [1280/1500 (83%)]\tLoss: 3.490167\n",
            "\n",
            "Test set: Average loss: 3.1258, Accuracy: 2069/10000 (21%)\n",
            "\n",
            "Train Epoch: 8 [0/1500 (0%)]\tLoss: 2.829136\n",
            "Train Epoch: 8 [640/1500 (42%)]\tLoss: 3.942806\n",
            "Train Epoch: 8 [1280/1500 (83%)]\tLoss: 3.610144\n",
            "\n",
            "Test set: Average loss: 3.0813, Accuracy: 2099/10000 (21%)\n",
            "\n",
            "Train Epoch: 9 [0/1500 (0%)]\tLoss: 2.984853\n",
            "Train Epoch: 9 [640/1500 (42%)]\tLoss: 3.634485\n",
            "Train Epoch: 9 [1280/1500 (83%)]\tLoss: 3.077341\n",
            "\n",
            "Test set: Average loss: 3.0519, Accuracy: 2120/10000 (21%)\n",
            "\n",
            "Train Epoch: 10 [0/1500 (0%)]\tLoss: 3.247827\n",
            "Train Epoch: 10 [640/1500 (42%)]\tLoss: 2.811884\n",
            "Train Epoch: 10 [1280/1500 (83%)]\tLoss: 3.350574\n",
            "\n",
            "Test set: Average loss: 3.0308, Accuracy: 2144/10000 (21%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hessian function"
      ],
      "metadata": {
        "id": "s3qMpR7Vva_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_hessian(model: nn.Module, loss: torch.Tensor):\n",
        "  # Hessian of the loss w.r.t. the parameters\n",
        "\n",
        "  optimizer = optim.Adadelta(model.parameters(), lr=1)\n",
        "  model = model.to(DEVICE)\n",
        "  model.eval()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  xs = optimizer.param_groups[0]['params']\n",
        "  ys = loss\n",
        "\n",
        "  grads = torch.autograd.grad(ys.to(DEVICE), xs, create_graph=True)\n",
        "\n",
        "  grads2 = []\n",
        "  for j, grad in enumerate(grads):\n",
        "    start = time.time()\n",
        "    print(f\"Calculating pairwise (every param, every layer) second-order derivatives for params of layer {j}...\")\n",
        "\n",
        "    grad = torch.reshape(grad, [-1])\n",
        "\n",
        "    grads2_tmp = []\n",
        "    for count, g in enumerate(grad):\n",
        "      percent_done = count / (len(grad))\n",
        "      if (count % 100 == 0):\n",
        "        print(f\"{percent_done:.2%} done...\")\n",
        "\n",
        "      grads2_tmp_tmp = []  # creating variable naming, innit?\n",
        "      for x in xs:\n",
        "        g2 = torch.autograd.grad(g, x, retain_graph=True)[0]\n",
        "        g2 = torch.reshape(g2, [-1])\n",
        "        grads2_tmp_tmp.append(g2.data.cpu())\n",
        "\n",
        "      flat = None\n",
        "      for second_orders in grads2_tmp_tmp:\n",
        "        if(flat is None):\n",
        "          flat = second_orders.flatten()\n",
        "        else:\n",
        "          flat = torch.concat((flat, second_orders.flatten()))\n",
        "\n",
        "      grads2_tmp.append(flat.numpy())\n",
        "    grads2 += grads2_tmp\n",
        "\n",
        "    print('Time used is ', time.time() - start)\n",
        "  return np.array(grads2)"
      ],
      "metadata": {
        "id": "GO56xIYdiWF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hessian over the training dataset"
      ],
      "metadata": {
        "id": "sYsWm7J1vmvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "for i, (data, targets) in enumerate(train_loader):\n",
        "  print(f\"Computing average loss on train set {(i+1)/len(train_loader):.2%}   \", end='\\r')\n",
        "  data = data.to(DEVICE)\n",
        "  targets = targets.to(DEVICE)\n",
        "\n",
        "  losses.append(loss_fn(model(data), targets))\n",
        "\n",
        "avg_loss = torch.mean(torch.stack(losses))\n",
        "print(f'Average Train Loss: {avg_loss.item()} \\n')\n",
        "\n",
        "hessian = get_hessian(model, avg_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zGHk_COk0VO",
        "outputId": "afb93493-04dd-4794-c297-ccd7a0a297f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 2.9914748668670654 \n",
            "\n",
            "Calculating pairwise (every param, every layer) second-order derivatives for params of layer 0...\n",
            "0.00% done...\n",
            "Time used is  0.5287890434265137\n",
            "Calculating pairwise (every param, every layer) second-order derivatives for params of layer 1...\n",
            "0.00% done...\n",
            "Time used is  0.045325517654418945\n",
            "Calculating pairwise (every param, every layer) second-order derivatives for params of layer 2...\n",
            "0.00% done...\n",
            "5.92% done...\n",
            "11.83% done...\n",
            "17.75% done...\n",
            "23.67% done...\n",
            "29.59% done...\n",
            "35.50% done...\n",
            "41.42% done...\n",
            "47.34% done...\n",
            "53.25% done...\n",
            "59.17% done...\n",
            "65.09% done...\n",
            "71.01% done...\n",
            "76.92% done...\n",
            "82.84% done...\n",
            "88.76% done...\n",
            "94.67% done...\n",
            "Time used is  152.2990574836731\n",
            "Calculating pairwise (every param, every layer) second-order derivatives for params of layer 3...\n",
            "0.00% done...\n",
            "Time used is  0.3349490165710449\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hessian[:4, :4]  # showing parts of the Hessian"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fG5a6mgxk184",
        "outputId": "b8a3195a-9ca6-4fae-9110-0f58f76855bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[14.285873 ,  9.79766  ,  6.301756 ,  8.839961 ],\n",
              "       [ 9.79766  ,  9.980309 ,  7.5023203,  5.972213 ],\n",
              "       [ 6.301756 ,  7.5023203,  9.037817 ,  3.3699284],\n",
              "       [ 8.83996  ,  5.972213 ,  3.3699286,  9.470606 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Determinant of Hessian"
      ],
      "metadata": {
        "id": "zGpZSHC8vx5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.det(torch.Tensor(hessian))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPxBzTu2k1_S",
        "outputId": "42962046-b368-443d-d008-dfa7ac6dfb6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-0.)"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now with (L2) regularization!\n",
        "\n"
      ],
      "metadata": {
        "id": "zR7otxw3nkEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_l2 = Net().to(DEVICE)\n",
        "optimizer_l2 = optim.Adadelta(model_l2.parameters(), lr=LR, weight_decay=1e-5)\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=GAMMA)\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  train(False, model_l2, DEVICE, train_loader, optimizer_l2, epoch)\n",
        "  test(model_l2, DEVICE, test_loader)\n",
        "  scheduler.step()\n",
        "\n",
        "torch.save(model_l2.state_dict(), \"mnist_cnn_small_l2.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEX3BRHKk2DD",
        "outputId": "5a3428a0-20e0-4ddf-fb45-95d074ca07a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/1500 (0%)]\tLoss: 7.257487\n",
            "Train Epoch: 1 [640/1500 (42%)]\tLoss: 4.341743\n",
            "Train Epoch: 1 [1280/1500 (83%)]\tLoss: 3.155448\n",
            "\n",
            "Test set: Average loss: 3.0490, Accuracy: 1153/10000 (12%)\n",
            "\n",
            "Train Epoch: 2 [0/1500 (0%)]\tLoss: 3.053784\n",
            "Train Epoch: 2 [640/1500 (42%)]\tLoss: 3.006741\n",
            "Train Epoch: 2 [1280/1500 (83%)]\tLoss: 2.776328\n",
            "\n",
            "Test set: Average loss: 2.5493, Accuracy: 1883/10000 (19%)\n",
            "\n",
            "Train Epoch: 3 [0/1500 (0%)]\tLoss: 2.250318\n",
            "Train Epoch: 3 [640/1500 (42%)]\tLoss: 2.344134\n",
            "Train Epoch: 3 [1280/1500 (83%)]\tLoss: 2.425436\n",
            "\n",
            "Test set: Average loss: 2.2084, Accuracy: 2586/10000 (26%)\n",
            "\n",
            "Train Epoch: 4 [0/1500 (0%)]\tLoss: 2.450996\n",
            "Train Epoch: 4 [640/1500 (42%)]\tLoss: 2.018063\n",
            "Train Epoch: 4 [1280/1500 (83%)]\tLoss: 2.030682\n",
            "\n",
            "Test set: Average loss: 1.9338, Accuracy: 3345/10000 (33%)\n",
            "\n",
            "Train Epoch: 5 [0/1500 (0%)]\tLoss: 2.039955\n",
            "Train Epoch: 5 [640/1500 (42%)]\tLoss: 1.861044\n",
            "Train Epoch: 5 [1280/1500 (83%)]\tLoss: 1.688241\n",
            "\n",
            "Test set: Average loss: 1.6923, Accuracy: 4324/10000 (43%)\n",
            "\n",
            "Train Epoch: 6 [0/1500 (0%)]\tLoss: 1.612473\n",
            "Train Epoch: 6 [640/1500 (42%)]\tLoss: 1.657676\n",
            "Train Epoch: 6 [1280/1500 (83%)]\tLoss: 1.431502\n",
            "\n",
            "Test set: Average loss: 1.4935, Accuracy: 5053/10000 (51%)\n",
            "\n",
            "Train Epoch: 7 [0/1500 (0%)]\tLoss: 1.697080\n",
            "Train Epoch: 7 [640/1500 (42%)]\tLoss: 1.509730\n",
            "Train Epoch: 7 [1280/1500 (83%)]\tLoss: 1.109163\n",
            "\n",
            "Test set: Average loss: 1.3192, Accuracy: 5714/10000 (57%)\n",
            "\n",
            "Train Epoch: 8 [0/1500 (0%)]\tLoss: 1.320971\n",
            "Train Epoch: 8 [640/1500 (42%)]\tLoss: 1.443218\n",
            "Train Epoch: 8 [1280/1500 (83%)]\tLoss: 0.911980\n",
            "\n",
            "Test set: Average loss: 1.1755, Accuracy: 6192/10000 (62%)\n",
            "\n",
            "Train Epoch: 9 [0/1500 (0%)]\tLoss: 1.419214\n",
            "Train Epoch: 9 [640/1500 (42%)]\tLoss: 1.263651\n",
            "Train Epoch: 9 [1280/1500 (83%)]\tLoss: 1.114804\n",
            "\n",
            "Test set: Average loss: 1.0795, Accuracy: 6500/10000 (65%)\n",
            "\n",
            "Train Epoch: 10 [0/1500 (0%)]\tLoss: 0.955217\n",
            "Train Epoch: 10 [640/1500 (42%)]\tLoss: 1.078005\n",
            "Train Epoch: 10 [1280/1500 (83%)]\tLoss: 1.300902\n",
            "\n",
            "Test set: Average loss: 1.0361, Accuracy: 6631/10000 (66%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "losses_l2 = []\n",
        "for i, (data, targets) in enumerate(train_loader):\n",
        "  print(f\"Computing average loss on train set {(i+1)/len(train_loader):.2%}   \", end='\\r')\n",
        "  data = data.to(DEVICE)\n",
        "  targets = targets.to(DEVICE)\n",
        "\n",
        "  losses_l2.append(loss_fn(model_l2(data), targets))\n",
        "\n",
        "avg_loss_l2 = torch.mean(torch.stack(losses_l2))\n",
        "print(f'Average Train Loss (L2 Regularized): {avg_loss_l2.item()} \\n')\n",
        "\n",
        "hessian_l2 = get_hessian(model_l2, avg_loss_l2)"
      ],
      "metadata": {
        "id": "lIL22moLfS4H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb747bcf-b0d9-4310-b4f1-f0f6c77913fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss (L2 Regularized): 0.9927076101303101 \n",
            "\n",
            "Calculating pairwise (every param, every layer) second-order derivatives for params of layer 0...\n",
            "0.00% done...\n",
            "Time used is  0.5337684154510498\n",
            "Calculating pairwise (every param, every layer) second-order derivatives for params of layer 1...\n",
            "0.00% done...\n",
            "Time used is  0.04964184761047363\n",
            "Calculating pairwise (every param, every layer) second-order derivatives for params of layer 2...\n",
            "0.00% done...\n",
            "5.92% done...\n",
            "11.83% done...\n",
            "17.75% done...\n",
            "23.67% done...\n",
            "29.59% done...\n",
            "35.50% done...\n",
            "41.42% done...\n",
            "47.34% done...\n",
            "53.25% done...\n",
            "59.17% done...\n",
            "65.09% done...\n",
            "71.01% done...\n",
            "76.92% done...\n",
            "82.84% done...\n",
            "88.76% done...\n",
            "94.67% done...\n",
            "Time used is  152.73879718780518\n",
            "Calculating pairwise (every param, every layer) second-order derivatives for params of layer 3...\n",
            "0.00% done...\n",
            "Time used is  0.33493995666503906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hessian_l2[:4, :4]"
      ],
      "metadata": {
        "id": "KOFhzTS_fS57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e0bed22-e701-4591-c19d-baed712fdc72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[7.906001 , 6.4523478, 4.5737157, 2.36314  ],\n",
              "       [6.4523478, 7.199847 , 5.814742 , 2.117704 ],\n",
              "       [4.5737157, 5.8147426, 6.8861847, 1.4781702],\n",
              "       [2.3631403, 2.117704 , 1.4781703, 1.3852652]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.det(torch.Tensor(hessian_l2))"
      ],
      "metadata": {
        "id": "gWkBeKL9fTAG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4a6522e-78ef-47a5-cec1-fb0084fabddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-0.)"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating eigenvalues of the L2 Hessian"
      ],
      "metadata": {
        "id": "-D8CmOwUwglF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some ask why.  We ask why not?"
      ],
      "metadata": {
        "id": "zpskmSCWjYeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "u, s, vh = np.linalg.svd(hessian_l2, full_matrices=False)"
      ],
      "metadata": {
        "id": "HIXZr7eqfTCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(s < 0).any()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zuj7O5D4oxc1",
        "outputId": "6fbd4643-34af-45b4-c1af-b784f0a9aecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basin Volume"
      ],
      "metadata": {
        "id": "bXSw3WJNEYoZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refer [this post](https://www.lesswrong.com/posts/QPqztHpToij2nx7ET/hessian-and-basin-volume) for context."
      ],
      "metadata": {
        "id": "X9qhoygEjDY5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Log of volume of unit n-ball"
      ],
      "metadata": {
        "id": "s2COq-KREhG3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're using [Stirling's formula for approximating the gamma function](https://en.wikipedia.org/wiki/Volume_of_an_n-ball#Approximation_for_high_dimensions) to not have to deal with very large factorials here from the large number of dimensions."
      ],
      "metadata": {
        "id": "YPCk-hmKibnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_v = math.log(1 / math.sqrt(num_params * np.pi)) + num_params / 2 * math.log(2 * np.pi * np.e / num_params)  # calculating volume directly gives us 0"
      ],
      "metadata": {
        "id": "TG5BvmC_ElON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Log of threshold term"
      ],
      "metadata": {
        "id": "jNryyja2BOs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 2"
      ],
      "metadata": {
        "id": "yBmZnvnHjRIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_threshold_term = num_params / 2 * math.log(2 * threshold)"
      ],
      "metadata": {
        "id": "c_yZSm7eBSdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## L2 Regularization term"
      ],
      "metadata": {
        "id": "eW3mvWwzIW5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wd = 1e-5  # weight decay\n",
        "k = 1\n",
        "init_std = 0.5\n",
        "c = k / (init_std ** 2)\n",
        "\n",
        "reg_term = torch.mul(wd + c, torch.eye(num_params))"
      ],
      "metadata": {
        "id": "LhRiGg3DIa2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Log of denominator"
      ],
      "metadata": {
        "id": "lHDCA_l3KB-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hessian_tmp = np.add(hessian_l2, reg_term)\n",
        "log_determinant = torch.logdet(hessian_tmp)  # calculating determinant directly give us infinity"
      ],
      "metadata": {
        "id": "Pvz8cBY9kncF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_denom = log_determinant * 1/2  # 1/2 * log(det) = log(det^1/2)"
      ],
      "metadata": {
        "id": "tVJ0isQZKFR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating (log of) basin volume"
      ],
      "metadata": {
        "id": "MO2-w4X-KqpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Log volume of unit ball: {}\".format(log_v))\n",
        "print(\"Log of threshold term: {}\".format(log_threshold_term))\n",
        "print(\"Log of denominator: {}\".format(log_denom))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhMtJ6w0Bl_d",
        "outputId": "6559a0fd-90fa-42cf-a59c-24dcb6be0ee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log volume of unit ball: -3942.742192807366\n",
            "Log of threshold term: 1185.2816787575064\n",
            "Log of denominator: 1190.5987548828125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_v_basin = log_v + log_threshold_term - log_denom"
      ],
      "metadata": {
        "id": "HDRQjjioAgTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_v_basin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4mQ2QciLEnE",
        "outputId": "2dd75e6a-2cbe-474e-a80b-816369e66680"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-3948.0591)"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    }
  ]
}