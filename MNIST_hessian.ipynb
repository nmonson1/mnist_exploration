{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfQ37idyU9pvWYKKVo/t6m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nmonson1/mnist_exploration/blob/main/MNIST_hessian.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZatMf5m-K7i"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 1, 3, 1)\n",
        "    self.fc2 = nn.Linear(169, 10)\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "    if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n",
        "      module.weight.data.normal_(mean=0.0, std=0.5)\n",
        "      if module.bias is not None:\n",
        "        module.bias.data.zero_()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = F.relu(x)\n",
        "    x = F.max_pool2d(x, 2)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.fc2(x)\n",
        "    output = F.log_softmax(x, dim=1)\n",
        "    return output"
      ],
      "metadata": {
        "id": "-v6pHEtB-NqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "TEST_BATCH_SIZE = 1000\n",
        "EPOCHS = 10\n",
        "LR = 1.0\n",
        "GAMMA = 0.7\n",
        "num_params = 1710"
      ],
      "metadata": {
        "id": "9nC7vVWU-Q-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "if use_cuda:\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "else:\n",
        "    DEVICE = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "KzUNjrjH-SzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net().to(DEVICE)\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=LR)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "x_MUPfls-S7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_kwargs = {'batch_size': BATCH_SIZE}\n",
        "test_kwargs = {'batch_size': TEST_BATCH_SIZE}\n",
        "if use_cuda:\n",
        "  cuda_kwargs = {'num_workers': 1, 'pin_memory': True, 'shuffle': True}\n",
        "  train_kwargs.update(cuda_kwargs)\n",
        "  test_kwargs.update(cuda_kwargs)\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "dataset1 = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
        "dataset2 = datasets.MNIST('../data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(Subset(dataset1, np.random.randint(len(dataset1), size=1500)), **train_kwargs)  # subset of datasets to keep network overparameterized\n",
        "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjj0nhSp-XD5",
        "outputId": "7763af20-bff5-4a9c-f556-8da27bde5222"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 88174176.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 16666991.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 26185251.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 11319387.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dry_run: bool, model, device, train_loader, optimizer, epoch):\n",
        "  model.train()\n",
        "\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = F.nll_loss(output, target)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch_idx % 10 == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item()))\n",
        "      if dry_run:\n",
        "        break\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "  model.eval()\n",
        "\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      output = model(data)\n",
        "      test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "      pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "id": "DgFvztmB-ZcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler = StepLR(optimizer, step_size=1, gamma=GAMMA)\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  train(False, model, DEVICE, train_loader, optimizer, epoch)\n",
        "  test(model, DEVICE, test_loader)\n",
        "  scheduler.step()\n",
        "\n",
        "torch.save(model.state_dict(), \"mnist_cnn_small.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hpj7xgJM-dmm",
        "outputId": "87d0afe7-492b-4164-8cb4-e27203335225"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/1500 (0%)]\tLoss: 34.024174\n",
            "Train Epoch: 1 [640/1500 (42%)]\tLoss: 20.776669\n",
            "Train Epoch: 1 [1280/1500 (83%)]\tLoss: 14.220222\n",
            "\n",
            "Test set: Average loss: 13.6152, Accuracy: 1240/10000 (12%)\n",
            "\n",
            "Train Epoch: 2 [0/1500 (0%)]\tLoss: 14.740343\n",
            "Train Epoch: 2 [640/1500 (42%)]\tLoss: 11.231129\n",
            "Train Epoch: 2 [1280/1500 (83%)]\tLoss: 8.936780\n",
            "\n",
            "Test set: Average loss: 8.9646, Accuracy: 1498/10000 (15%)\n",
            "\n",
            "Train Epoch: 3 [0/1500 (0%)]\tLoss: 9.846889\n",
            "Train Epoch: 3 [640/1500 (42%)]\tLoss: 8.023938\n",
            "Train Epoch: 3 [1280/1500 (83%)]\tLoss: 6.735827\n",
            "\n",
            "Test set: Average loss: 6.9281, Accuracy: 1711/10000 (17%)\n",
            "\n",
            "Train Epoch: 4 [0/1500 (0%)]\tLoss: 7.620111\n",
            "Train Epoch: 4 [640/1500 (42%)]\tLoss: 6.407550\n",
            "Train Epoch: 4 [1280/1500 (83%)]\tLoss: 5.571795\n",
            "\n",
            "Test set: Average loss: 5.8107, Accuracy: 1870/10000 (19%)\n",
            "\n",
            "Train Epoch: 5 [0/1500 (0%)]\tLoss: 6.363405\n",
            "Train Epoch: 5 [640/1500 (42%)]\tLoss: 5.472382\n",
            "Train Epoch: 5 [1280/1500 (83%)]\tLoss: 4.874392\n",
            "\n",
            "Test set: Average loss: 5.1312, Accuracy: 1994/10000 (20%)\n",
            "\n",
            "Train Epoch: 6 [0/1500 (0%)]\tLoss: 5.590352\n",
            "Train Epoch: 6 [640/1500 (42%)]\tLoss: 4.887510\n",
            "Train Epoch: 6 [1280/1500 (83%)]\tLoss: 4.437873\n",
            "\n",
            "Test set: Average loss: 4.6941, Accuracy: 2078/10000 (21%)\n",
            "\n",
            "Train Epoch: 7 [0/1500 (0%)]\tLoss: 5.089147\n",
            "Train Epoch: 7 [640/1500 (42%)]\tLoss: 4.505931\n",
            "Train Epoch: 7 [1280/1500 (83%)]\tLoss: 4.154229\n",
            "\n",
            "Test set: Average loss: 4.4041, Accuracy: 2140/10000 (21%)\n",
            "\n",
            "Train Epoch: 8 [0/1500 (0%)]\tLoss: 4.760693\n",
            "Train Epoch: 8 [640/1500 (42%)]\tLoss: 4.250907\n",
            "Train Epoch: 8 [1280/1500 (83%)]\tLoss: 3.960157\n",
            "\n",
            "Test set: Average loss: 4.2067, Accuracy: 2183/10000 (22%)\n",
            "\n",
            "Train Epoch: 9 [0/1500 (0%)]\tLoss: 4.536875\n",
            "Train Epoch: 9 [640/1500 (42%)]\tLoss: 4.074183\n",
            "Train Epoch: 9 [1280/1500 (83%)]\tLoss: 3.826651\n",
            "\n",
            "Test set: Average loss: 4.0700, Accuracy: 2210/10000 (22%)\n",
            "\n",
            "Train Epoch: 10 [0/1500 (0%)]\tLoss: 4.383776\n",
            "Train Epoch: 10 [640/1500 (42%)]\tLoss: 3.950915\n",
            "Train Epoch: 10 [1280/1500 (83%)]\tLoss: 3.732811\n",
            "\n",
            "Test set: Average loss: 3.9738, Accuracy: 2226/10000 (22%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_hessian(model: nn.Module, loss: torch.Tensor):\n",
        "  # Hessian of the loss w.r.t. the parameters\n",
        "\n",
        "  optimizer = optim.Adadelta(model.parameters(), lr=1)\n",
        "  model = model.to(DEVICE)\n",
        "  model.eval()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  xs = optimizer.param_groups[0]['params']\n",
        "  ys = loss\n",
        "\n",
        "  grads = torch.autograd.grad(ys.to(DEVICE), xs, create_graph=True)\n",
        "\n",
        "  grads2 = []\n",
        "  for j, grad in enumerate(grads):\n",
        "    start = time.time()\n",
        "    print(f\"Calculating pairwise (every param, every layer) second-order derivatives for params of layer {j}...\")\n",
        "\n",
        "    grad = torch.reshape(grad, [-1])\n",
        "\n",
        "    grads2_tmp = []\n",
        "    for count, g in enumerate(grad):\n",
        "      percent_done = count / (len(grad))\n",
        "      if (count % 100 == 0):\n",
        "        print(f\"{percent_done:.2%} done...\")\n",
        "\n",
        "      grads2_tmp_tmp = []  # creating variable naming, innit?\n",
        "      for x in xs:\n",
        "        g2 = torch.autograd.grad(g, x, retain_graph=True)[0]\n",
        "        g2 = torch.reshape(g2, [-1])\n",
        "        grads2_tmp_tmp.append(g2.data.cpu())\n",
        "\n",
        "      flat = None\n",
        "      for second_orders in grads2_tmp_tmp:\n",
        "        if(flat is None):\n",
        "          flat = second_orders.flatten()\n",
        "        else:\n",
        "          flat = torch.concat((flat, second_orders.flatten()))\n",
        "\n",
        "      grads2_tmp.append(flat.numpy())\n",
        "    grads2 += grads2_tmp\n",
        "\n",
        "    print('Time used is ', time.time() - start)\n",
        "  return np.array(grads2)"
      ],
      "metadata": {
        "id": "lE_Au4r5-gPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "for i, (data, targets) in enumerate(train_loader):\n",
        "  print(f\"Computing average loss on train set {(i+1)/len(train_loader):.2%}   \", end='\\r')\n",
        "  data = data.to(DEVICE)\n",
        "  targets = targets.to(DEVICE)\n",
        "\n",
        "  losses.append(loss_fn(model(data), targets))\n",
        "\n",
        "avg_loss = torch.mean(torch.stack(losses))\n",
        "print(f'Average Train Loss: {avg_loss.item()} \\n')\n",
        "\n",
        "hessian = get_hessian(model, avg_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uBHkcVf-iuR",
        "outputId": "c035f153-ef1c-4807-b86f-9b52ad007fd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 3.8263282775878906 \n",
            "\n",
            "Calculating pairwise (every param, every layer) second-order derivatives for params of layer 0...\n",
            "0.00% done...\n",
            "Time used is  3.1911449432373047\n",
            "Calculating pairwise (every param, every layer) second-order derivatives for params of layer 1...\n",
            "0.00% done...\n",
            "Time used is  0.11049914360046387\n",
            "Calculating pairwise (every param, every layer) second-order derivatives for params of layer 2...\n",
            "0.00% done...\n",
            "5.92% done...\n",
            "11.83% done...\n",
            "17.75% done...\n",
            "23.67% done...\n",
            "29.59% done...\n",
            "35.50% done...\n",
            "41.42% done...\n",
            "47.34% done...\n",
            "53.25% done...\n",
            "59.17% done...\n",
            "65.09% done...\n",
            "71.01% done...\n",
            "76.92% done...\n",
            "82.84% done...\n",
            "88.76% done...\n",
            "94.67% done...\n",
            "Time used is  243.7737901210785\n",
            "Calculating pairwise (every param, every layer) second-order derivatives for params of layer 3...\n",
            "0.00% done...\n",
            "Time used is  0.9884436130523682\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hessian.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XtKe7AqAoMg",
        "outputId": "0f68e7fc-2691-463e-eaa5-a3d8bdf1d07b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1710, 1710)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Psn0c1VAAs9P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}